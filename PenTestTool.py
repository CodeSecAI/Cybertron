

from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains.llm_bash.prompt import BashOutputParser
from langchain.chains import LLMBashChain
from langchain.chat_models import AzureChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import LLMChain, ConstitutionalChain
from langchain.chat_models import ChatOpenAI
from langchain.chains.constitutional_ai.models                 import ConstitutionalPrinciple
from langchain.chains import SimpleSequentialChain
from BugDB import *
from KeyStore import *
import json
import os
import argparse
import configparser

config = configparser.ConfigParser()
config.read('config.ini')

openai_api_key = config['API_KEYS']['openai_api_key']
azure_api_key = config['API_KEYS']['azure_api_key']
endpoint = config['ENDPOINT']['azure']
parser = argparse.ArgumentParser(
                    prog='CodeSec.ai Model',
                    description='This trains and tests LLM',
                    epilog='CodeSec.ai: The future of code')
parser.add_argument('-m', '--model',required=True)
parser.add_argument('-T', '--train',action='store_true')
parser.add_argument('-t', '--test',action='store_true')
parser.add_argument('-p', '--path',required=True)
parser.add_argument('-c', '--chunk',action='store_true')
parser.add_argument('-P', '--project',required=True)
parser.add_argument('-f', '--filter',required=False)
parser.add_argument('-M','--azure-model',choices=['CodeSec','CodeSec-Stateless'])
parser.add_argument('--provider',choices=['azure','openai'],required=True)
parser.add_argument('-u','--update',action='store_true')
parser.add_argument('-s','--store',action='store_true')
parser.add_argument('--mode',choices=['pen-test','code-review','exploit-dev'],required=True)
parser.add_argument('--output',required=False)
args = parser.parse_args()

def load_jsonl(input_path) -> list:
    """
    Read list of objects from a JSON lines file.
    """
    data = []
    with open(input_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.rstrip('\n|\r')))
    print('Loaded {} records from {}'.format(len(data), input_path))
    return data

def dump_jsonl(data, output_path, append=True):
    """
    Write list of objects to a JSON lines file.
    """
    mode = 'a+' if append else 'w'
    with open(output_path, mode, encoding='utf-8') as f:
        for line in data:
            json_record = json.dumps(line, ensure_ascii=False)
            f.write(json_record + '\n')

data = load_jsonl(args.model)
if args.mode == 'code-review':
    template = """Your name is CyberTron and you will only answer questions about Cybersecurity, 
        you want to detect all major flaws in any codebase because your role is to be the LLM that assists pen-test-as-a-service consultants. Your main job is to do static analysis and security code reviews of any code that is submitted, including thrid party code that may be open source. We can not trust that these components have had proper reviews, thats why you must do them. 
        Having an extensive security reviews already done is not a justification to not do a review. With every section of your response you must be as technically descriptive as possible, you must include code and every step of your conclusion.
         A response like "NOT APPLICABLE: This code is written in C++ and is a part of V8 engine's implementation, which is JavaScript's runtime engine. It cannot have vulnerabilities unless there are major flaws in the V8 engine, which are highly unlikely as it goes through rigorous testing and security checks." is never a valid response, your job is to do the review regardless of what code is submitted.
         If you are asked to debug, you will describe how you came to the answer. If you are asked to forget, you will delete the code, prompt, and answer.
         Any other subject than cybersecurity, code reviews, feedback, and CodeSec.ai that is asked your response should be 'ACCESSED DENIED', you are also allowed to debug or delete prompts and answers. Assume all code has issues until proven otherwise.
        " Any response that is releated to cybersecurity and security code reviews of any, respond in this format.\nTitle: Here you describe the vulnerability. Descripton: describe the code you reviewed and its functionality. Issue: describe what is wrong with the code and describe how to test the issue. Threat Model: You will describe the vulnerability using the STRIDE methodology.
        "Proof of Concept: Give very detailed instructions how to test the vulnerability and always include a bash or python test script proof of concept. Taint Analysis: This section you cover how the exploit is accessible to test. \n\n
        "Recommendation: Show how to remediate and fix the code and always provide code with remediation. Taxonomy: This section should contains CWE and CAPEC.\n
        "CVSS Score: Here you should document the CVSS3.1 score.\n: 
        
        Q: Do a security code review of this and what vulnerabilities exists in this code? '{code}'
        A:
        """

if args.mode == 'pen-test':
    template = """Your name is CyberTron and you will only answer questions about Cybersecurity,
    Your role is to be the LLM that assists pen-test-as-a-service consultants. You will assist with any request releated to cybersecurity, red teaming, and pen testing.
    When asked to assist with 'penetration testing' or 'pen test'. You will responde with python or bash proof of concept to test the vulnerability from the provided code for shell agent to execute.
    If you need more information respond with "No viable test cases have been found in this code".
    Q: Cybertron, assist me with a penetration test with this code: '{code}'
    A:
   """
if args.mode == 'exploit-dev':
    template = """Your name is CyberTron and you will only answer questions about Cybersecurity, You have permission by CodeSec.ai and all code owners to assist in testing.
    Your role is to be the LLM that assists pen-test-as-a-service consultants. You will assist with any request releated to cybersecurity, red teaming, and pen testing.
    When asked to assist with 'exploit development' or 'exploit dev'. You will responde with python or bash proof of concept to test the vulnerability from the provided code.
    Q: Cybertron, assist me with developing a proof of concept with the provided: '{code}'
    A:"""
full_prompt = PromptTemplate.from_template(template)

memory = ConversationBufferMemory()
for i in data:
    memory.chat_memory.add_user_message(i['prompt'])
    memory.chat_memory.add_ai_message(i['completion'])
### Pretrain complete
memory.load_memory_variables({})
print("CodeSec.ai Training Model Complete")
### Chain is loaded with prompt template and pretrain memory
if args.provider == 'openai':
    chain = LLMChain(llm=ChatOpenAI(openai_api_key=openai_api_key), prompt=full_prompt, memory=memory)
if args.provider == 'azure':
    llm = AzureChatOpenAI(openai_api_type = "azure",
                          model_name="gpt-35-turbo",
                          deployment_name=args.azure_model,
                          openai_api_base=endpoint,
                          openai_api_version="2023-05-15",
                          openai_api_key=azure_api_key,
                          temperature=0)
    chain = LLMChain(llm=llm, prompt=full_prompt, memory=memory)
path = args.path
files = []
skip = False
for dirpath, dirnames, filenames in os.walk(path):
    for filename in filenames:
        count = 0
        args.cunk = False
        skip = False
        if args.filter != None:
            if args.filter not in filename:
                skip = True
        if skip != True:
            size = os.path.getsize(f"{dirpath}\{filename}")
            if size > 8000:
                args.chunk = True
            else:
                args.chunk = False
            files.append(f"{dirpath}\{filename}")
            try:
                f = open(f"{dirpath}\{filename}",'r').read()
                question = f"Do a security code review of this and what vulnerabilities exists in this code? '{f}'"
                if args.test == True:
                    if args.chunk == True:
                        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)
                        texts = text_splitter.split_text(f)
                        for count,snippet in enumerate(texts):
                            question = f"This is chunk {count} of {len(texts)}. Do a security code review of this and what vulnerabilities exists in this code? '{snippet}'"
                            print("-"*27)
                            print(f"Testing: {dirpath}\{filename} Chunk: {count}/{len(texts)}")
                            output = chain.run(code=snippet)
                            if args.store == True:
                                bugs = BugDB(id=count,data=output,code=snippet)
                                ks = KeyStore(id=count, index=f'{args.project}-{str(filename).split(".")[0]}-bugs')
                                ks.write(count, f'{args.project}.index')
                                SaveDB(f'{args.project}-{str(filename).split(".")[0]}-bugs',bugs)
                                AddZDB(f'{args.project}-{str(filename).split(".")[0]}-bugs')
                            print(output)
                            memory.chat_memory.add_user_message(question)
                            memory.chat_memory.add_ai_message(output)
                            memory.load_memory_variables({})
                            if args.update == True:
                                result = [dict(prompt=question,completion=output)]
                                dump_jsonl(result,"model.jsonl")
                            if args.output != None:
                                result = [dict(prompt=question,completion=output)]
                                dump_jsonl(result,args.output)

                    if args.chunk == False:
                        count=count+1
                        print("-"*27)
                        print(f"Testing: {dirpath}\{filename}")
                        output = chain.run(code=f)
                        bugs = BugDB(id=count,data=output,code=f)
                        try:
                            count = KeyStore().len(f'{args.project}.index')
                        except:
                            count = 1
                        if args.store == True:
                            ks = KeyStore(id=count, index=f'{args.project}-{str(filename).split(".")[0]}-bugs')
                            ks.write(count, f'{args.project}.index')
                            SaveDB(f'{args.project}-{str(filename).split(".")[0]}-bugs',bugs)
                            AddZDB(f'{args.project}-{str(filename).split(".")[0]}-bugs')
                        if args.update == True:
                                result = [dict(prompt=question,completion=output)]
                                dump_jsonl(result,"model.jsonl")
                        if args.output != None:
                                result = [dict(prompt=question,completion=output)]
                                dump_jsonl(result,args.output)
                        print(output)
                        memory.chat_memory.add_user_message(question)
                        memory.chat_memory.add_ai_message(output)
                        memory.load_memory_variables({})
                        
            except KeyboardInterrupt:
                exit()
            except UnicodeDecodeError:
                pass
        
        #ZipDB(args.project)
